{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pingouin"
      ],
      "metadata": {
        "id": "5Ep3_86VO9-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyampute"
      ],
      "metadata": {
        "id": "_kCpJ2P5Oy91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y xgboost\n",
        "!pip install xgboost==2.1.2"
      ],
      "metadata": {
        "id": "vDM9BtIqnM53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "NcXyaPvkwTZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW48yCnFt8bQ"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "EP RQ2 Model Final\n",
        "\n",
        "Author: Joseph Toland; Validation: Anchal Bansal\n",
        "\"\"\"\n",
        "\n",
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "import io\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "from scipy.stats import ttest_1samp, spearmanr\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import (roc_auc_score, accuracy_score, confusion_matrix, roc_curve, auc)\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from shapely.geometry import Point\n",
        "\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from google.colab import files\n",
        "\n",
        "# Setting seaborn style\n",
        "sns.set(style='white')  # white, darkgrid\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload a CSV file using files.upload()\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the first uploaded file (assuming it's a CSV)4\n",
        "uploaded_file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Create a DataFrame from the uploaded CSV file\n",
        "source = pd.read_csv(io.StringIO(uploaded[uploaded_file_name].decode('utf-8')))"
      ],
      "metadata": {
        "id": "DIpVUmZhLA3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Survey Validation"
      ],
      "metadata": {
        "id": "SfPOw0-vLx7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the CSV\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the first uploaded file (assuming it's a CSV)4\n",
        "uploaded_file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Create a DataFrame from the uploaded CSV file\n",
        "source_CN = pd.read_csv(io.StringIO(uploaded[uploaded_file_name].decode('utf-8')))\n",
        "\n",
        "source_CN.dropna(inplace = True) #to check a clean version with all NA removed for Cronback and rWg. Use: SURVEY_ANALYSIS_FINAL_RQ2_101224_CRONBACH.csv\n",
        "source_CN['P-WSAFETY-10'] = pd.to_numeric(source_CN['P-WSAFETY-10'], errors='coerce')\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the first uploaded file (assuming it's a CSV)4\n",
        "uploaded_file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Create a DataFrame from the uploaded CSV file #to check missingness with NO NA removed. Use: SURVEY_ANALYSIS_FINAL_RQ2_101224_TESTS.csv\n",
        "source_T = pd.read_csv(io.StringIO(uploaded[uploaded_file_name].decode('utf-8')))"
      ],
      "metadata": {
        "id": "hrbfTVtGOwZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Little MCAR Test  "
      ],
      "metadata": {
        "id": "NQFeD3uX-oBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Little MCAR TEST\n",
        "import pyampute as pyampute\n",
        "from pyampute.exploration.mcar_statistical_tests import MCARTest\n",
        "\n",
        "# Conduct the MCAR test\n",
        "mt = MCARTest(method=\"little\")\n",
        "p_value = mt.little_mcar_test(source_T)  # This should directly return the p-value.  Note be sure to use the data from BEFORE demogrpahic varibles have been matched to categorical reference categories.\n",
        "\n",
        "# Print the p-value\n",
        "print(\"P-Value from Little's MCAR Test:\", p_value)\n",
        "\n",
        "# Interpret the p-value\n",
        "if p_value < 0.05:\n",
        "    print(\"The missing data is not MCAR (reject the null hypothesis).\")\n",
        "else:\n",
        "    print(\"The missing data can be considered MCAR (fail to reject the null hypothesis).\")\n"
      ],
      "metadata": {
        "id": "gQ88j0S9Mzfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Cronback Alpha Test"
      ],
      "metadata": {
        "id": "Eq-8uunp-x9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pingouin import cronbach_alpha\n",
        "\n",
        "WSAFE = source_CN[['P-WSAFETY-1', 'P-WSAFETY-2', 'P-WSAFETY-3', 'P-WSAFETY-4',\n",
        "       'P-WSAFETY-5', 'P-WSAFETY-6', 'P-WSAFETY-7', 'P-WSAFETY-8',\n",
        "       'P-WSAFETY-9', 'P-WSAFETY-10', 'P-WSAFETY-11']]\n",
        "\n",
        "EQF = source_CN[['P-SOC-EQF-1', 'P-SOC-EQF-2', 'P-SOC-EQF-3', 'P-SOC-EQF-4']]\n",
        "\n",
        "TRUST = source_CN[['P-TRUST-1', 'P-TRUST-2', 'P-TRUST-3', 'P-TRUST-4', 'P-TRUST-5',\n",
        "       'P-TRUST-6', 'P-TRUST-7', 'P-TRUST-8', 'P-TRUST-9', 'P-TRUST-10',\n",
        "       'P-TRUST-11', 'P-TRUST-12', 'P-TRUST-13', 'P-TRUST-14', 'P-TRUST-15',\n",
        "       'P-TRUST-16', 'P-TRUST-17', 'P-TRUST-18', 'P-TRUST-19']]\n",
        "\n",
        "alpha_result = cronbach_alpha(WSAFE)\n",
        "print(\"Cronbach's Alpha for W-SAFETY:\", round(alpha_result[0], 2))\n",
        "\n",
        "alpha_result = cronbach_alpha(TRUST)\n",
        "print(\"Cronbach's Alpha for W-TRUST:\", round(alpha_result[0], 2))\n",
        "\n",
        "alpha_result = cronbach_alpha(EQF)\n",
        "print(\"Cronbach's Alpha for P-SOC-EQF:\", round(alpha_result[0], 2))"
      ],
      "metadata": {
        "id": "OvHMD2jXOOel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Sample, Two-Sided T-Test for the RwG Test"
      ],
      "metadata": {
        "id": "bzOkBXekPh06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the hypothesized mean\n",
        "hypothesized_mean = 3\n",
        "\n",
        "# Create a dictionary to store the results and list of DataFrames\n",
        "results = {}\n",
        "dataframes = [WSAFE, EQF, TRUST]\n",
        "\n",
        "# Perform one-sample t-test for each DataFrame\n",
        "for df_label, df in zip(['WSAFE', 'EQF', 'TRUST'], dataframes):\n",
        "    # Create a new DataFrame for each column\n",
        "    df_columns = {col: df[col] for col in df.columns}\n",
        "\n",
        "    # Remove NA values for each column\n",
        "    df_no_na = {col: col_data.dropna() for col, col_data in df_columns.items()}\n",
        "\n",
        "    # Perform the one-sample t-test for each column again\n",
        "    t_statistic = []\n",
        "    p_value = []\n",
        "    for col, col_data in df_no_na.items():\n",
        "        t_stat, p_val = ttest_1samp(col_data, hypothesized_mean)\n",
        "        t_statistic.append(t_stat)\n",
        "        p_value.append(p_val)\n",
        "\n",
        "    # Store the results in the dictionary\n",
        "    results[df_label] = {'t_statistic': t_statistic, 'p_value': p_value}\n",
        "\n",
        "# Create a DataFrame from the results dictionary\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Apply the styling to the 'p_value' column\n",
        "styled_results = results_df.style.applymap(lambda x: highlight_significance(x) if 'p_value' in x else '')\n",
        "\n",
        "# Display the styled DataFrame. This take some interpretation, we find all p values greater than 0.05 for investigation that fail the test. Then associated the predictor based on the order.  These are then selected for RwG test.\n",
        "styled_results\n"
      ],
      "metadata": {
        "id": "VdRhoX_CM2ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rWG Within Group Reliability Test"
      ],
      "metadata": {
        "id": "JuQHhc158hRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate rWG for Likert scale variables without grouping\n",
        "\n",
        "variable_columns = ['P-WSAFETY-1', 'P-WSAFETY-4', 'P-WSAFETY-5','P-TRUST-1', 'P-TRUST-3'] #these must be from the p values that are high above where we compare them all to see if there is a central tendency selection bias\n",
        "\n",
        "def calculate_rWG(data, variable_columns):\n",
        "    # Step 1: Remove NaN values from the specified columns\n",
        "    cleaned_data = data[variable_columns].dropna()\n",
        "\n",
        "    # Step 2: Calculate Total Variance for Likert scale variables\n",
        "    total_variance = np.var(cleaned_data.values.flatten(), ddof=1)\n",
        "\n",
        "    # Step 3: Calculate Within-Group Variance for Likert scale variables\n",
        "    within_group_variance = cleaned_data.apply(lambda x: ((x - x.mean())**2).sum()).mean()\n",
        "\n",
        "    # Step 4: Calculate rWG for Likert scale variables\n",
        "    k = len(variable_columns)\n",
        "    rWG = within_group_variance / (total_variance + (k - 1) * within_group_variance)\n",
        "\n",
        "    return rWG\n",
        "\n",
        "# Calculate rWG for all variables together, removing NaN values\n",
        "rWG_value = calculate_rWG(source_CN, variable_columns)\n",
        "\n",
        "# Print the result\n",
        "print(f'rWG Value for all variables: {rWG_value}')\n"
      ],
      "metadata": {
        "id": "jKpd04D8MtfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class imbalance test"
      ],
      "metadata": {
        "id": "b_xMpxoM1XST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check for class imbalance in all binary/categorical columns\n",
        "threshold_ratio = 0.1  # Threshold for imbalance (10:1 imbalance)\n",
        "\n",
        "# Iterate through each column in the dataset\n",
        "for column in source.columns:\n",
        "    # Skip non-binary columns\n",
        "    if source[column].nunique() > 2:\n",
        "        print(f\"Skipping column '{column}' (not binary/categorical).\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nChecking imbalance for column: {column}\")\n",
        "\n",
        "    # Get the distribution of classes\n",
        "    class_distribution = source[column].value_counts()\n",
        "    print(\"Class distribution:\")\n",
        "    print(class_distribution)\n",
        "\n",
        "    # Calculate imbalance ratio\n",
        "    imbalance_ratio = class_distribution.min() / class_distribution.max()\n",
        "\n",
        "    # Print whether the column exceeds the imbalance threshold\n",
        "    if imbalance_ratio < threshold_ratio:\n",
        "        print(f\"Imbalance ratio: {imbalance_ratio:.2f}. This exceeds the 10:1 threshold, indicating class imbalance.\")\n",
        "    else:\n",
        "        print(f\"Imbalance ratio: {imbalance_ratio:.2f}. This does not exceed the 10:1 threshold, indicating no class imbalance.\")\n"
      ],
      "metadata": {
        "id": "ocXVTUFx4g0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UWu1JbIgiqK"
      },
      "source": [
        "#Descriptive Statistics, Chi-Sqares Tests, and Corelations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descriptive Statistics"
      ],
      "metadata": {
        "id": "dnM1KGKK1CjS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KmLB_XNgmNm"
      },
      "outputs": [],
      "source": [
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "mean_std_min_max_summary = source.describe().loc[['mean', 'std', 'min', 'max']].round(2)\n",
        "print(mean_std_min_max_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-square tests"
      ],
      "metadata": {
        "id": "pkK1U0Gz1GoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_CHI = source\n",
        "\n",
        "source_CHI.dropna(inplace=True)  # Clean version with all NAs removed (n = 181).\n",
        "\n",
        "# Select categorical variables for Chi-square tests\n",
        "chi = source_CHI[['R-WINSEC', 'P-OWNRENT', 'P-WWELL', 'P-WMUNICIPAL', 'P-WCHANGE',\n",
        "                  'P-WRELIEF', 'P-WPURCHASE', 'P-RISK-W', 'P-WTREAT', 'P-ACTION-R',\n",
        "                  'P-EDU-HS', 'P-INCOME-50', 'P-GENDER', 'P-WORK-Y']]\n",
        "\n",
        "variables = chi.columns.tolist()\n",
        "results = pd.DataFrame(columns=variables, index=variables)\n",
        "\n",
        "for i, var1 in enumerate(variables):\n",
        "    for j, var2 in enumerate(variables):\n",
        "        if i > j:\n",
        "            contingency_table = pd.crosstab(chi[var1], chi[var2])\n",
        "            chi2, p, _, _ = stats.chi2_contingency(contingency_table)\n",
        "            results.at[var1, var2] = round(p, 2)\n",
        "        elif i == j:  # On the diagonal\n",
        "            results.at[var1, var2] = np.nan\n",
        "\n",
        "# Highlight p-values\n",
        "def highlight_associated(val):\n",
        "    if np.isnan(val):\n",
        "        return \"color: rgba(0, 128, 0, 0)\"\n",
        "    if val <= 0.01:\n",
        "        return \"color: red\"\n",
        "    if val <= 0.05:\n",
        "        return \"color: orange\"\n",
        "    return \"\"\n",
        "\n",
        "# Style and display the DataFrame\n",
        "styled_results = results.style.format(precision=2).applymap(highlight_associated)\n",
        "styled_results\n"
      ],
      "metadata": {
        "id": "RLmy8BZbaiYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spearman Rank Correlation"
      ],
      "metadata": {
        "id": "FI6nPiXf1KDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting only the Likert and all other non-categorical variables\n",
        "spear = source[['P-SOC-EQF','P-WSAFETY','P-TRUST','P-HH-POP', 'P-HH-CHILD','P-WQUANT-G', 'P-WQUANT-EM']] #note, chldren and large families are correlated, as are water stoarge and eemergneyc storage for SI\n",
        "\n",
        "# Calculating Spearman's rank correlation coefficients\n",
        "correlations = spear.corr(method='spearman')\n",
        "\n",
        "# Calculating p-values for correlations\n",
        "p_values = spear.corr(method=lambda x, y: spearmanr(x, y)[1])[['P-SOC-EQF','P-WSAFETY','P-TRUST','P-HH-POP', 'P-HH-CHILD','P-WQUANT-G', 'P-WQUANT-EM']]\n",
        "\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "print(\"Correlations:\")\n",
        "print(correlations[['P-SOC-EQF','P-WSAFETY','P-TRUST','P-HH-POP', 'P-HH-CHILD','P-WQUANT-G','P-WQUANT-EM']])\n",
        "print(\"\\nP-values:\")\n",
        "print(p_values)\n"
      ],
      "metadata": {
        "id": "0BL7y9rSLapU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Exploration: BoxPlots"
      ],
      "metadata": {
        "id": "Kz6gqH764Bip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming 'source' is your DataFrame\n",
        "source_BOXBI = source.copy()\n",
        "source_BOXBI.dropna(inplace=True)  # Drop rows with missing values\n",
        "source_BOXBI = source_BOXBI.drop(['INDEX'], axis=1)  # Drop the 'INDEX' column\n",
        "\n",
        "# Mapping of variable codes to their simplified descriptions\n",
        "variable_mapping = {\n",
        "    'R-HWISE': 'HWISE score',\n",
        "    'R-WINSEC': 'Water insecure (HWISE)',\n",
        "    'P-WWELL': 'Household normally used\\nprivate well water',\n",
        "    'P-WMUNICIPAL': 'Household normally used\\nmunicipal water',\n",
        "    'P-WCHANGE': 'Change in water source',\n",
        "    'P-WRELIEF': 'Used donated bottled\\nwater (after)',\n",
        "    'P-WPURCHASE': 'Purchased bottled\\nwater (after)',\n",
        "    'P-HH-POP': 'Number of people in\\nhousehold',\n",
        "    'P-HH-CHILD': 'Number of children\\nin household',\n",
        "    'P-INCOME-50': 'Income (< 50K)',\n",
        "    'P-OWNRENT': 'Tenure (Owner)',\n",
        "    'P-EDU-HS': 'Education (12 years or less)',\n",
        "    'P-WORK-Y': 'Employment (Working full\\nor part time)',\n",
        "    'P-GENDER': 'Gender (Female)',\n",
        "    'P-AGE-65': 'Age (> 65 years)',\n",
        "    'P-WSAFETY': 'Perceptions of water safety',\n",
        "    'P-TRUST': 'Perceptions of trust',\n",
        "    'P-SOC-EQF': 'Perceptions of equity\\nand fairness',\n",
        "    'P-RISK-W': 'Told drinking water\\nwas unsafe',\n",
        "    'P-WTREAT': 'Installed a home water treatment\\nsystem or filters',\n",
        "    'P-ACTION-R': 'Relocation',\n",
        "    'P-WQUANT-EM': 'Surplus/ Deficit emergency\\ndrinking water',\n",
        "    'P-WQUANT-G': 'Water storage (gallons)',\n",
        "    'P-INT-1': 'Interaction: Water safety and\\nchange',\n",
        "    'P-INT-2': 'Interaction: Water safety,\\nchange, and used\\ndonated water'\n",
        "}\n",
        "\n",
        "# Rename the columns in the DataFrame\n",
        "source_BOXBI.rename(columns=variable_mapping, inplace=True)\n",
        "\n",
        "# Updated list of variables to plot (after renaming)\n",
        "variables = list(variable_mapping.values())\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(18, 18))  # 5 rows, 5 columns for 25 plots\n",
        "\n",
        "# Flatten the axes to make it easier to loop through\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Loop through each variable and create a boxplot\n",
        "for i, var in enumerate(variables):\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Create box plot for each variable\n",
        "    source_BOXBI.boxplot(column=var, ax=ax, patch_artist=True,  # Patch for color fill\n",
        "                         boxprops=dict(facecolor='lightgrey', color='black'),  # Box color\n",
        "                         whiskerprops=dict(color='black'),  # Whisker color\n",
        "                         capprops=dict(color='black'))  # Cap color\n",
        "\n",
        "\n",
        "    # Set only the Y-axis label\n",
        "    ax.set_ylabel('Values', fontsize=12)  # Y-axis label\n",
        "\n",
        "    # Remove the x-axis label by not setting it\n",
        "    ax.set_xlabel(None)\n",
        "\n",
        "    # Optionally adjust y-axis to fit data better (if needed)\n",
        "    ax.set_ylim(source_BOXBI[var].min() - 1, source_BOXBI[var].max() + 1)\n",
        "\n",
        "# Adjust layout to prevent overlap and give space for titles\n",
        "fig.tight_layout()\n",
        "plt.subplots_adjust(top=0.90)  # Adjust the space at the top to avoid title overlap\n",
        "\n",
        "# Customize font and style for publication\n",
        "sns.set_style(\"whitegrid\")  # Use a clean, professional background\n",
        "plt.rcParams.update({'font.size': 10, 'font.family': 'serif'})  # Update font size and style\n",
        "\n",
        "# Save the plot as a high-quality image (300 DPI) for publication\n",
        "plt.savefig(\"boxplot_matrix.png\", dpi=600, bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SXauoJtgpSZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Model"
      ],
      "metadata": {
        "id": "8pbR_Ah8dTJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process data"
      ],
      "metadata": {
        "id": "9ddIxJ2x3u_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_LR = source.drop(['INDEX'],axis=1)\n",
        "source_LR.dropna(inplace = True) #to check a clean version with all drop these to get to 181:"
      ],
      "metadata": {
        "id": "Q3BFsASbdSOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Xs, and Y\n",
        "X = source_LR.drop(columns=['R-WINSEC', 'R-HWISE','P-INT-1', 'P-INT-2'])  #add or remove interaction effects\n",
        "Y = source_LR[['R-WINSEC']]\n",
        "\n",
        "columns_to_scale = ['P-WQUANT-G', 'P-WQUANT-EM', 'P-WSAFETY', 'P-TRUST','P-SOC-EQF' ] # We scale these columns so all values of coefficent are comparable by magnitude\n",
        "\n",
        "# Initialize MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Scale the specified columns\n",
        "X[columns_to_scale] = scaler.fit_transform(X[columns_to_scale])\n"
      ],
      "metadata": {
        "id": "GsizI2okd0WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logisitc regression model evaluation"
      ],
      "metadata": {
        "id": "D5hQdpt94ChX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_vif(X):\n",
        "    \"\"\"Calculate Variance Inflation Factor (VIF) for each feature.\"\"\"\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Feature\"] = X.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
        "    return vif_data\n",
        "\n",
        "def backward_selection(X, target, significance_level=0.1, max_iter=1000):\n",
        "    \"\"\"Perform backward feature selection with logistic regression.\"\"\"\n",
        "    remaining_features = list(X.columns)\n",
        "    step = 1\n",
        "\n",
        "    while remaining_features:\n",
        "        X_subset = sm.add_constant(X[remaining_features])  # Add constant term\n",
        "        logit_model = sm.Logit(target, X_subset)\n",
        "        result = logit_model.fit(maxiter=max_iter, disp=0)  # Fit model with max iterations\n",
        "\n",
        "        p_values = result.pvalues[1:]  # Exclude constant term\n",
        "        max_p_value = p_values.max()\n",
        "        max_p_index = p_values.idxmax()\n",
        "\n",
        "        vif_data = calculate_vif(X[remaining_features])\n",
        "        vif = vif_data.loc[vif_data['Feature'] == max_p_index, 'VIF'].values[0]\n",
        "\n",
        "        if max_p_value > significance_level:\n",
        "            X_temp = X_subset.drop(columns=[max_p_index])\n",
        "            logit_model_temp = sm.Logit(target, X_temp)\n",
        "            result_temp = logit_model_temp.fit(maxiter=max_iter, disp=0)\n",
        "            new_AIC = result_temp.aic\n",
        "\n",
        "            print(f\"Step {step}: Removing feature: {max_p_index}, p-value: {max_p_value:.4f}, VIF: {vif:.2f}, AIC without: {new_AIC:.2f}\")\n",
        "\n",
        "            # Calculate ROC curve for the temporary model\n",
        "            predicted_probs = result_temp.predict(X_temp)\n",
        "            fpr, tpr, _ = roc_curve(target, predicted_probs)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            remaining_features.remove(max_p_index)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        step += 1\n",
        "\n",
        "    # Final model summary and plotting\n",
        "    X_final = sm.add_constant(X[remaining_features])\n",
        "    final_model = sm.Logit(target, X_final)\n",
        "    result = final_model.fit()\n",
        "\n",
        "    print(result.summary())\n",
        "\n",
        "    marginal_effects = result.get_margeff(at='overall')\n",
        "\n",
        "    # Print out the summary of the marginal effects\n",
        "    marginal_effects_summary = marginal_effects.summary()\n",
        "    print(\"\\nMarginal Effects for Remaining Variables in Final Model:\")\n",
        "    print(marginal_effects_summary)\n",
        "\n",
        "    coefficients = result.params\n",
        "    conf_int = result.conf_int()\n",
        "\n",
        "    #Define names\n",
        "    defined_names = [\n",
        "        'Constant',\n",
        "        'Tenure (Owner)',\n",
        "        'Change in water\\nsource',\n",
        "        'Used donated bottled\\n water (after)',\n",
        "        'Water storage',\n",
        "        'Perceptions of water\\nsafety',\n",
        "        'Relocation',\n",
        "        'Gender (female)',\n",
        "        'Age (> 65)',\n",
        "    ]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "\n",
        "    # Define colors for bars\n",
        "    colors = ['#B4CADE'] * len(coefficients)\n",
        "    if 'P-GENDER' in coefficients.index:\n",
        "        colors[coefficients.index.get_loc('P-GENDER')] = '#E8E8E8'\n",
        "    if 'P-ACTION-R' in coefficients.index:\n",
        "        colors[coefficients.index.get_loc('P-ACTION-R')] = '#E8E8E8'\n",
        "\n",
        "    if len(defined_names) != len(coefficients):\n",
        "        print(\"Warning: Length of defined names does not match length of coefficients.\")\n",
        "\n",
        "    coefficients.index = defined_names[:len(coefficients)]\n",
        "    plt.barh(coefficients.index, coefficients.values, color=colors, label='Coefficient')\n",
        "\n",
        "    # Plot confidence intervals\n",
        "    plt.errorbar(coefficients.values, coefficients.index,\n",
        "                 xerr=[coefficients.values - conf_int[0], conf_int[1] - coefficients.values],\n",
        "                 fmt='o', color='black', label='Confidence Interval')\n",
        "\n",
        "    plt.axvline(x=0, color='.5')\n",
        "    plt.xlabel('Coefficient Value')\n",
        "    plt.title('Coefficients of Logistic Regression Model')\n",
        "    plt.ylabel('Features')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # Fit model with all features for ROC curve\n",
        "    X_1 = sm.add_constant(X)\n",
        "    logit_model = sm.Logit(target, X_1)\n",
        "    result = logit_model.fit()\n",
        "\n",
        "    predicted_probs = result.predict(X_1)\n",
        "    fpr, tpr, _ = roc_curve(target, predicted_probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('FIGURE1.png', dpi=600, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nVIF of remaining features:\")\n",
        "    vif_data = calculate_vif(X[remaining_features])\n",
        "    print(vif_data)\n",
        "\n",
        "    return result, remaining_features\n",
        "\n",
        "result, remaining_features = backward_selection(X, Y, max_iter=1000)\n"
      ],
      "metadata": {
        "id": "LTnESXFufPik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run train-test validation for LR"
      ],
      "metadata": {
        "id": "6hLtGK9o4-Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    return accuracy, sensitivity, specificity\n",
        "\n",
        "def evaluate_model(X_train, X_test, Y_train, Y_test, remaining_features):\n",
        "    X_train_const = sm.add_constant(X_train[remaining_features])\n",
        "    final_model = sm.Logit(Y_train, X_train_const)\n",
        "    result = final_model.fit(disp=0)\n",
        "\n",
        "    X_test_const = sm.add_constant(X_test[remaining_features])\n",
        "    y_pred_probs = result.predict(X_test_const)\n",
        "    y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "\n",
        "    accuracy, sensitivity, specificity = compute_metrics(Y_test, y_pred)\n",
        "\n",
        "    # Calculate ROC curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(Y_test, y_pred_probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    return accuracy, sensitivity, specificity, fpr, tpr, roc_auc\n",
        "\n",
        "def run_experiments(X, Y, remaining_features, num_experiments=100):\n",
        "    random_states = np.random.randint(1, 1001, size=num_experiments)\n",
        "    accuracies, sensitivities, specificities = [], [], []\n",
        "    aucs = []\n",
        "    all_fpr = []\n",
        "    all_tpr = []\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    for random_state in random_states:\n",
        "        try:\n",
        "            train_x, test_x, train_y, test_y = train_test_split(\n",
        "                X, Y,\n",
        "                test_size=0.5,\n",
        "                random_state=random_state,\n",
        "                stratify=Y\n",
        "            )\n",
        "            accuracy, sensitivity, specificity, fpr, tpr, roc_auc = evaluate_model(train_x, test_x, train_y, test_y, remaining_features)\n",
        "            accuracies.append(accuracy)\n",
        "            sensitivities.append(sensitivity)\n",
        "            specificities.append(specificity)\n",
        "            aucs.append(roc_auc)\n",
        "\n",
        "            all_fpr.append(fpr)\n",
        "            all_tpr.append(tpr)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with random state {random_state}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if len(accuracies) == 0:\n",
        "        print(\"No successful experiments.\")\n",
        "        return\n",
        "\n",
        "    # Calculate average ROC\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    mean_tpr = np.mean([np.interp(mean_fpr, fpr, tpr) for fpr, tpr in zip(all_fpr, all_tpr)], axis=0)\n",
        "\n",
        "    # Calculate upper and lower bounds\n",
        "    lower_tpr = np.min([np.interp(mean_fpr, fpr, tpr) for fpr, tpr in zip(all_fpr, all_tpr)], axis=0)\n",
        "    upper_tpr = np.max([np.interp(mean_fpr, fpr, tpr) for fpr, tpr in zip(all_fpr, all_tpr)], axis=0)\n",
        "\n",
        "    # Finalize and show ROC plot\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line\n",
        "    plt.plot(mean_fpr, mean_tpr, color='darkorange', lw=2, label='Mean ROC curve (AUC = {:.2f})'.format(np.mean(aucs)))  # Average ROC curve\n",
        "    plt.fill_between(mean_fpr, lower_tpr, upper_tpr, color='lightgrey', alpha=0.5, label='ROC curve area')  # Fill area between upper and lower curves\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Average Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('FIGURE2.png', dpi=600, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Print average metrics\n",
        "    avg_accuracy = np.mean(accuracies)\n",
        "    avg_sensitivity = np.mean(sensitivities)\n",
        "    avg_specificity = np.mean(specificities)\n",
        "    avg_auc = np.mean(aucs)\n",
        "\n",
        "    std_accuracy = np.std(accuracies)\n",
        "    std_sensitivity = np.std(sensitivities)\n",
        "    std_specificity = np.std(specificities)\n",
        "    std_auc = np.std(aucs)\n",
        "\n",
        "    print(f\"Average Accuracy: {avg_accuracy:.2f} (±{std_accuracy:.2f})\")\n",
        "    print(f\"Average Sensitivity: {avg_sensitivity:.2f} (±{std_sensitivity:.2f})\")\n",
        "    print(f\"Average Specificity: {avg_specificity:.2f} (±{std_specificity:.2f})\")\n",
        "    print(f\"Average AUC: {avg_auc:.2f} (±{std_auc:.2f})\")\n",
        "\n",
        "run_experiments(X, Y, remaining_features)"
      ],
      "metadata": {
        "id": "F16GbQ8Fa51r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP for comparison table between XGBoost and logisitc regression"
      ],
      "metadata": {
        "id": "uVjVa1wu-ubb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Define names for features (used in outputs)\n",
        "defined_names = [\n",
        "    'Constant',\n",
        "    'Tenure (Owner)',\n",
        "    'Change in water\\nsource',\n",
        "    'Used donated bottled\\nwater (after)',\n",
        "    'Water storage',\n",
        "    'Perceptions of water\\nsafety',\n",
        "    'Relocation',\n",
        "    'Gender (female)',\n",
        "    'Age (> 65)',\n",
        "]\n",
        "\n",
        "def calculate_vif(X):\n",
        "    \"\"\"Calculate Variance Inflation Factor (VIF) for each feature.\"\"\"\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Feature\"] = X.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
        "    return vif_data\n",
        "\n",
        "def backward_selection(X, target, significance_level=0.1, max_iter=1000):\n",
        "    \"\"\"Perform backward feature selection with logistic regression.\"\"\"\n",
        "    remaining_features = list(X.columns)\n",
        "    step = 1\n",
        "\n",
        "    while remaining_features:\n",
        "        X_subset = sm.add_constant(X[remaining_features])  # Add constant term\n",
        "        logit_model = sm.Logit(target, X_subset)\n",
        "        result = logit_model.fit(maxiter=max_iter, disp=0)  # Fit model with max iterations\n",
        "\n",
        "        p_values = result.pvalues[1:]  # Exclude constant term\n",
        "        max_p_value = p_values.max()\n",
        "        max_p_index = p_values.idxmax()\n",
        "\n",
        "        vif_data = calculate_vif(X[remaining_features])\n",
        "        vif = vif_data.loc[vif_data['Feature'] == max_p_index, 'VIF'].values[0]\n",
        "\n",
        "        if max_p_value > significance_level:\n",
        "            X_temp = X_subset.drop(columns=[max_p_index])\n",
        "            logit_model_temp = sm.Logit(target, X_temp)\n",
        "            result_temp = logit_model_temp.fit(maxiter=max_iter, disp=0)\n",
        "            new_AIC = result_temp.aic\n",
        "\n",
        "            feature_index = X.columns.get_loc(max_p_index)\n",
        "            feature_name = defined_names[feature_index] if feature_index < len(defined_names) else max_p_index\n",
        "\n",
        "            print(f\"Step {step}: Removing feature: {feature_name}, p-value: {max_p_value:.4f}, VIF: {vif:.2f}, AIC without: {new_AIC:.2f}\")\n",
        "\n",
        "            remaining_features.remove(max_p_index)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        step += 1\n",
        "\n",
        "    # Final model summary and plotting\n",
        "    X_final = sm.add_constant(X[remaining_features])\n",
        "    final_model = sm.Logit(target, X_final)\n",
        "    result = final_model.fit()\n",
        "\n",
        "    print(result.summary())\n",
        "\n",
        "    # Use SHAP to explain the final logistic regression model\n",
        "    def predict_function(X):\n",
        "        \"\"\"Create a callable prediction function for SHAP.\"\"\"\n",
        "        return result.predict(sm.add_constant(X))\n",
        "\n",
        "    explainer = shap.Explainer(predict_function, X_final)  # Use the callable function\n",
        "    shap_values = explainer(X_final)\n",
        "\n",
        "    # Create a summary plot (beeswarm plot)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.summary_plot(shap_values, X_final, show=False)\n",
        "\n",
        "    # Define the custom labels for the SHAP plot\n",
        "    custom_labels = [\n",
        "        'Constant',\n",
        "        'Water storage',\n",
        "        'Relocation',\n",
        "        'Age (> 65)',\n",
        "        'Gender (female)',\n",
        "        'Used donated bottled\\nwater (after)',\n",
        "        'Change in water\\nsource',\n",
        "        'Tenure (Owner)',\n",
        "        'Perceptions of water\\nsafety'\n",
        "    ]\n",
        "\n",
        "    # Modify y-tick labels after the plot is created\n",
        "    num_features = len(custom_labels)\n",
        "    plt.yticks(ticks=range(num_features), labels=custom_labels)\n",
        "\n",
        "    # Save the SHAP summary plot (beeswarm plot)\n",
        "    plt.savefig('shap_summary_beeswarm_plot.png', dpi=600, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Optionally initialize JavaScript for SHAP plots\n",
        "    shap.initjs()\n",
        "\n",
        "    return result, remaining_features\n",
        "\n",
        "# Assuming you have already defined X and Y as your features and target\n",
        "result, remaining_features = backward_selection(X, Y, max_iter=1000)\n"
      ],
      "metadata": {
        "id": "s3X8B7N8-tFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost Model"
      ],
      "metadata": {
        "id": "l_zzN1_S1fxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data processing"
      ],
      "metadata": {
        "id": "RAF6sL3d0T3z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMyEWpdO2PWM"
      },
      "outputs": [],
      "source": [
        "#Final veriables slected based on research questions and hypotheses in paper, base don n = 222 full dataset in source\n",
        "X = source.drop(columns=['INDEX','R-WINSEC', 'R-HWISE','P-INT-1', 'P-INT-2'])\n",
        "Y = source[['R-WINSEC']]\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.1,random_state= 15,  stratify=source['R-WINSEC'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost Model Evaluation"
      ],
      "metadata": {
        "id": "uxm6qsRSpWyJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7C2z-4YAAlG"
      },
      "outputs": [],
      "source": [
        "# Define and initialize the XGBoost model, based on hyperparameter selection with some final tuning\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=2,\n",
        "    objective='binary:logistic',\n",
        "    subsample=1,\n",
        "    gamma=0,\n",
        "    reg_lambda=1,\n",
        "    colsample_bytree=0.8,\n",
        "    colsample_bylevel=0.8,\n",
        ")\n",
        "\n",
        "# Perform 10-fold cross-validation to evaluate the model\n",
        "xgb_cv_scores = cross_val_score(xgb_model, train_x, train_y, cv=10, scoring='roc_auc')\n",
        "print(\"XGBoost Train AUC (CV): {0:.2f} (+/- {1:.2f})\".format(xgb_cv_scores.mean(), xgb_cv_scores.std() * 2))\n",
        "\n",
        "# Fit the model on the training data\n",
        "xgb_model.fit(train_x, train_y)\n",
        "\n",
        "# Make predictions and calculate metrics\n",
        "xgb_train_predictions = xgb_model.predict(train_x)\n",
        "xgb_test_prob_predictions = xgb_model.predict_proba(test_x)[:, 1]\n",
        "xgb_full_model_predictions = xgb_model.predict_proba(X)[:, 1]\n",
        "xgb_full_model_binary_predictions = (xgb_full_model_predictions > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "train_accuracy = accuracy_score(train_y, xgb_train_predictions)\n",
        "test_auc = roc_auc_score(test_y, xgb_test_prob_predictions)\n",
        "full_model_auc = roc_auc_score(Y, xgb_full_model_predictions)\n",
        "full_model_accuracy = accuracy_score(Y, xgb_full_model_binary_predictions)\n",
        "test_accuracy = accuracy_score(test_y, xgb_model.predict(test_x))\n",
        "\n",
        "# Calculate sensitivity and specificity for the full model\n",
        "conf_matrix = confusion_matrix(Y, xgb_full_model_binary_predictions)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "# Print results\n",
        "print(\"XGBoost Training Accuracy: {0:.2f}\".format(train_accuracy))\n",
        "print(\"XGBoost Test AUC: {0:.2f}\".format(test_auc))\n",
        "print(\"XGBoost Test Accuracy: {0:.2f}\".format(test_accuracy))\n",
        "print(\"XGBoost Full Model AUC: {0:.2f}\".format(full_model_auc))\n",
        "print(\"XGBoost Full Model Accuracy: {0:.2f}\".format(full_model_accuracy))\n",
        "print(\"XGBoost Full Model Sensitivity: {0:.2f}\".format(sensitivity))\n",
        "print(\"XGBoost Full Model Specificity: {0:.2f}\".format(specificity))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym05l-zQhCvQ"
      },
      "source": [
        "XGBoost Visualization - ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWtIKfMwQ3wj"
      },
      "outputs": [],
      "source": [
        "# Make probability predictions on the full dataset\n",
        "xgb_probs_full = xgb_model.predict_proba(X)[:, 1]\n",
        "\n",
        "# Calculate the false positive rate, true positive rate, and thresholds\n",
        "fpr_full, tpr_full, thresholds_full = roc_curve(Y, xgb_probs_full)\n",
        "\n",
        "# Calculate the AUC for the full dataset\n",
        "roc_auc_full = auc(fpr_full, tpr_full)\n",
        "\n",
        "# Plot the ROC curve for the full dataset\n",
        "plt.figure()\n",
        "plt.plot(fpr_full, tpr_full, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_full)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) - Full Dataset')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('ROC_XGBt.png', dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print the AUC for the full dataset\n",
        "print(\"XGBoost Full Dataset AUC: {0:.2f}\".format(roc_auc_full))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP Feature Importance Summary Plot"
      ],
      "metadata": {
        "id": "3qVD_KoK2cuo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xecrOvWdRFCG"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "# Create a SHAP explainer object\n",
        "explainer = shap.Explainer(xgb_model)\n",
        "\n",
        "# Calculate SHAP values for all features on the full dataset X\n",
        "shap_values = explainer.shap_values(X)\n",
        "\n",
        "# Create a summary plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.summary_plot(shap_values, X, show=False)\n",
        "\n",
        "# Define specific names for each feature\n",
        "custom_labels = [\n",
        "  'Private well',\n",
        "\t'Municipal water',\n",
        "\t'Education\\n(high school or less)',\n",
        "\t'Home water treatment',\n",
        "\t'Gender (female)',\n",
        "\t'Children',\n",
        "\t'Income (< 50K)',\n",
        "\t'Perceptions of\\nequity',\n",
        "\t'Told water unsafe',\n",
        "\t'Relocation',\n",
        "\t'Used donated bottled\\nwater (after)',\n",
        "\t'Perceptions of trust',\n",
        "  'Emergency water',\n",
        "\t'Change in water\\nsource',\n",
        "\t'Household size',\n",
        "\t'Age (> 65)',\n",
        "\t'Water storage',\n",
        "\t'Purchased bottled\\nwater (after)',\n",
        "\t'Tenure (Owner)',\n",
        "\t'Perceptions of water\\nsafety'\n",
        "]\n",
        "\n",
        "# Modify y-tick labels after the plot is created\n",
        "num_features = len(custom_labels)\n",
        "plt.yticks(ticks=range(num_features), labels=custom_labels)\n",
        "\n",
        "# Save the SHAP summary plot (beeswarm plot)\n",
        "plt.savefig('shap_summary_beeswarm_plot.png', dpi=600, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Optionally initialize JavaScript\n",
        "shap.initjs()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permuation Importance Test"
      ],
      "metadata": {
        "id": "kzIdB3IF08Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perm_importance = permutation_importance(xgb_model, X, Y, n_repeats=100, random_state=15) # run n iterations for permutation importance\n",
        "\n",
        "# Feature names from the dataset\n",
        "default_labels = X.columns.tolist()\n",
        "\n",
        "# Custom labels for specific features\n",
        "custom_labels = {\n",
        "    'P-WORK-Y': 'Employment',\n",
        "    'P-WMUNICIPAL': 'Municipal water',\n",
        "    'P-WWELL': 'Private well',\n",
        "    'P-EDU-HS': 'Education\\n(high school or less)',\n",
        "    'P-GENDER': 'Gender (female)',\n",
        "    'P-WTREAT': 'Home water treatment',\n",
        "    'P-INCOME-50': 'Income (< 50K)',\n",
        "    'P-HH-CHILD': 'Children',\n",
        "    'P-RISK-W': 'Told water unsafe',\n",
        "    'P-AGE-65': 'Age (> 65)',\n",
        "    'P-TRUST': 'Perceptions of trust',\n",
        "    'P-SOC-EQF': 'Perceptions of equity',\n",
        "    'P-ACTION-R': 'Relocation',\n",
        "    'P-WCHANGE': 'Change in water source',\n",
        "    'P-WRELIEF': 'Used donated bottled\\nwater (after)',\n",
        "    'P-WQUANT-EM': 'Emergency water',\n",
        "    'P-WPURCHASE': 'Purchased bottled\\nwater (after)',\n",
        "    'P-HH-POP': 'Household size',\n",
        "    'P-WQUANT-G': 'Water storage',\n",
        "    'P-OWNRENT': 'Tenure (Owner)',\n",
        "    'P-WSAFETY': 'Perceptions of water\\nsafety'\n",
        "}\n",
        "\n",
        "# Creating labels for the plot using the custom mapping\n",
        "plot_labels = [custom_labels.get(col, col) for col in default_labels]\n",
        "\n",
        "# Plotting the permutation importance\n",
        "sorted_idx = perm_importance.importances_mean.argsort()\n",
        "plt.figure(figsize=(9, 9))\n",
        "bars = plt.barh(range(len(sorted_idx)), perm_importance.importances_mean[sorted_idx],\n",
        "                 xerr=perm_importance.importances_std[sorted_idx], align='center', color='#B5CBDF')\n",
        "\n",
        "# Adding custom legend\n",
        "plt.yticks(range(len(sorted_idx)), np.array(plot_labels)[sorted_idx])  # Use mapped labels\n",
        "plt.xlabel(\"Permutation Importance\")\n",
        "plt.title(\"Permutation Importance of Features\")\n",
        "\n",
        "mean_importance_patch = mpatches.Rectangle((0, 0), width=0.2, height=0.1, color='#B5CBDF', edgecolor='black', label='Global importance (mean)')\n",
        "std_dev_patch = plt.Line2D([0, 1], [0, 0], color='black', linestyle='-', lw=1, label='± Std. Dev.')  # Dashed line for Std Dev\n",
        "\n",
        "# Add both patches to the legend\n",
        "plt.legend(handles=[mean_importance_patch, std_dev_patch], loc=\"lower right\")\n",
        "\n",
        "plt.savefig('PERM_XGB.png', dpi=600, bbox_inches='tight')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xobxp-MV4ZbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partial dependnace plots"
      ],
      "metadata": {
        "id": "tRgQS-fC21jH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "# Create a custom colormap from blue (#0000F5) to red (#EA3323)\n",
        "colors = ['#0000F5', '#EA3323']\n",
        "cmap = LinearSegmentedColormap.from_list('custom_cmap', colors)\n",
        "\n",
        "# Create a SHAP explainer object and calculate SHAP values\n",
        "explainer = shap.Explainer(xgb_model)\n",
        "shap_values = explainer.shap_values(X)\n",
        "\n",
        "# Choose the feature of interest\n",
        "feature = 'P-WSAFETY'\n",
        "interaction_feature = 'P-WCHANGE'\n",
        "\n",
        "# Get the values of the feature and the interaction feature\n",
        "feature_values = X[feature]\n",
        "interaction_values = X[interaction_feature]\n",
        "\n",
        "# Compute the SHAP values\n",
        "shap_feature_values = shap_values[:, X.columns.get_loc(feature)]\n",
        "\n",
        "# Sort the data\n",
        "sorted_idx = np.argsort(feature_values)\n",
        "sorted_feature_values = feature_values.iloc[sorted_idx]\n",
        "sorted_shap_values = shap_feature_values[sorted_idx]\n",
        "\n",
        "# Create a figure with two subplots (one for SHAP Partial Dependence plot, and the other for the stacked bar chart)\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# --- SHAP Partial Dependence Plot ---\n",
        "scatter = axs[0].scatter(\n",
        "    sorted_feature_values, sorted_shap_values,\n",
        "    c=interaction_values.iloc[sorted_idx], cmap=cmap, alpha=1\n",
        ")\n",
        "\n",
        "# Set plot labels, title, and axis ticks for the SHAP plot\n",
        "axs[0].set_xlabel('Perceptions of Water Safety (High-Low)', fontsize=14, labelpad=15)\n",
        "axs[0].set_ylabel('SHAP Value (Impact on Model)', fontsize=14, labelpad=15)\n",
        "axs[0].set_title(f'Partial Dependence Plot of Change in Water Source\\n (Interaction with Perception of Water Safety)', fontsize=16, pad=20)\n",
        "axs[0].tick_params(axis='both', labelsize=12)\n",
        "axs[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "axs[0].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(scatter, ax=axs[0])\n",
        "cbar.set_label('Change in Water Source', fontsize=12)\n",
        "cbar.set_ticks(np.arange(int(np.min(interaction_values)), int(np.max(interaction_values)) + 1, 1))\n",
        "\n",
        "# --- Stacked Bar Chart with Percentage Labels ---\n",
        "# Create a contingency table (counts) of the two features of interest\n",
        "contingency_table = pd.crosstab(X['P-WSAFETY'], X['P-WCHANGE'])\n",
        "\n",
        "# Calculate the percentage contribution for each group\n",
        "percentages = contingency_table.div(contingency_table.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Plot the stacked bar chart on the second subplot using the custom colormap\n",
        "contingency_table.plot(kind='bar', stacked=True, colormap=cmap, ax=axs[1])\n",
        "\n",
        "# Customize the chart with labels and title\n",
        "axs[1].set_xlabel('Perceptions of Water Safety (High-Low)', fontsize=14)\n",
        "axs[1].set_ylabel('Count', fontsize=14)\n",
        "axs[1].set_title('Partial Dependence Contribution', fontsize=16)\n",
        "axs[1].tick_params(axis='both', labelsize=12)\n",
        "axs[1].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "axs[1].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "# Add percentage labels to the red parts (last column of contingency table)\n",
        "for i, (group, row) in enumerate(contingency_table.iterrows()):\n",
        "    # Get the cumulative height of all previous bars (for positioning)\n",
        "    cumulative_height = 0\n",
        "    for j, value in enumerate(row):\n",
        "        if j == len(row) - 1:  # If it's the last column (red part)\n",
        "            percentage = percentages.iloc[i, j]\n",
        "            axs[1].text(\n",
        "                i, cumulative_height + value / 2,  # Position: center of the red bar\n",
        "                f'{percentage:.0f}%',  # Label text\n",
        "                ha='center', va='center', color='black', fontsize=10, alpha = 0\n",
        "            )\n",
        "        cumulative_height += value\n",
        "\n",
        "handles, labels = axs[1].get_legend_handles_labels()\n",
        "\n",
        "# Manually update the legend labels, e.g., '0' -> 'No', '1' -> 'Yes'\n",
        "updated_labels = ['No' if label == '0' else 'Yes' for label in labels]\n",
        "\n",
        "# Set the new legend with the updated labels\n",
        "axs[1].legend(handles, updated_labels, title='Change in Water Source', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('INTS.png', dpi=600, bbox_inches='tight')\n",
        "\n",
        "# Show the combined plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DgM4WFbRbVNU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}